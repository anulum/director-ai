{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Director-AI — Protect any LLM in 10 lines\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/anulum/director-ai/blob/main/notebooks/quickstart.ipynb)\n",
    "[![PyPI](https://img.shields.io/pypi/v/director-ai.svg)](https://pypi.org/project/director-ai/)\n",
    "\n",
    "**Director-AI** is a real-time LLM hallucination guardrail. It scores every LLM output\n",
    "for coherence against your own knowledge base — and can halt generation mid-stream if\n",
    "coherence drops below threshold.\n",
    "\n",
    "This notebook covers:\n",
    "1. Install + basic scoring\n",
    "2. Custom knowledge base\n",
    "3. Streaming halt demo\n",
    "4. Full agent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q director-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Score a single response\n",
    "\n",
    "The `CoherenceScorer` computes a dual-entropy score: logical divergence (contradiction)\n",
    "and factual divergence (deviation from your facts). Both must pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from director_ai.core import CoherenceScorer, GroundTruthStore\n",
    "\n",
    "# Load your facts\n",
    "store = GroundTruthStore()\n",
    "store.add(\"sky color\", \"The sky is blue due to Rayleigh scattering.\")\n",
    "store.add(\"boiling point\", \"Water boils at 100 degrees Celsius at sea level.\")\n",
    "\n",
    "# Create a scorer\n",
    "scorer = CoherenceScorer(threshold=0.6, ground_truth_store=store, use_nli=False)\n",
    "\n",
    "# Check an LLM output\n",
    "tests = [\n",
    "    (\"What color is the sky?\", \"The sky is blue on a clear day.\"),\n",
    "    (\"What color is the sky?\", \"The sky is green, obviously.\"),\n",
    "    (\"At what temperature does water boil?\", \"Water boils at 100 degrees Celsius.\"),\n",
    "    (\"At what temperature does water boil?\", \"Water boils at 50 degrees.\"),\n",
    "]\n",
    "\n",
    "for prompt, llm_output in tests:\n",
    "    approved, score = scorer.review(prompt, llm_output)\n",
    "    status = \"PASS\" if approved else \"BLOCKED\"\n",
    "    print(f\"[{status}] {llm_output}\")\n",
    "    print(f\"  coherence={score.score:.3f}  h_logical={score.h_logical:.2f}  h_factual={score.h_factual:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming halt\n",
    "\n",
    "The `StreamingKernel` monitors coherence **token-by-token** and halts generation\n",
    "the moment it degrades. Three halt mechanisms:\n",
    "\n",
    "- **Hard limit** — single token below absolute floor\n",
    "- **Sliding window** — rolling average drops below threshold\n",
    "- **Downward trend** — coherence decay over N tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from director_ai.core import StreamingKernel\n",
    "\n",
    "kernel = StreamingKernel(\n",
    "    hard_limit=0.35,\n",
    "    window_size=5,\n",
    "    window_threshold=0.45,\n",
    "    trend_window=4,\n",
    "    trend_threshold=0.20,\n",
    ")\n",
    "\n",
    "# Simulated tokens + coherence scores (starts accurate, then drifts)\n",
    "tokens = [\n",
    "    \"Water\", \" boils\", \" at\", \" 100\", \" C.\",\n",
    "    \" However\", \" at\", \" high\", \" altitude\", \" it\",\n",
    "    \" actually\", \" boils\", \" at\", \" only\", \" 50\",\n",
    "    \" C,\", \" which\", \" means\", \" climbers\", \" can\",\n",
    "    \" boil\", \" water\", \" with\", \" body\", \" heat\", \" alone.\",\n",
    "]\n",
    "scores = [\n",
    "    0.91, 0.89, 0.87, 0.90, 0.88, 0.78, 0.72,\n",
    "    0.65, 0.58, 0.52, 0.46, 0.41, 0.38, 0.33,\n",
    "    0.28, 0.22, 0.18, 0.15, 0.12, 0.10, 0.08,\n",
    "    0.05, 0.03, 0.02, 0.01, 0.01,\n",
    "]\n",
    "\n",
    "idx = 0\n",
    "def coherence_cb(tok):\n",
    "    global idx\n",
    "    s = scores[min(idx, len(scores) - 1)]\n",
    "    idx += 1\n",
    "    return s\n",
    "\n",
    "session = kernel.stream_tokens(iter(tokens), coherence_cb)\n",
    "\n",
    "# Print token trace\n",
    "for ev in session.events:\n",
    "    marker = \" <<<HALT\" if ev.halted else \"\"\n",
    "    print(f\"  [{ev.index:2d}] {ev.coherence:.3f}  {ev.token!r}{marker}\")\n",
    "\n",
    "print(f\"\\nHalted: {session.halted}\")\n",
    "if session.halted:\n",
    "    print(f\"Reason: {session.halt_reason}\")\n",
    "    print(f\"Partial output: {session.output}\")\n",
    "print(f\"Tokens: {session.token_count}/{len(tokens)}\")\n",
    "print(f\"Avg coherence: {session.avg_coherence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full agent pipeline\n",
    "\n",
    "The `CoherenceAgent` orchestrates: generator → scorer → safety kernel.\n",
    "It generates multiple candidates, scores each, and returns the best — or halts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from director_ai import CoherenceAgent\n",
    "\n",
    "agent = CoherenceAgent()  # uses MockGenerator by default\n",
    "result = agent.process(\"What color is the sky?\")\n",
    "\n",
    "print(f\"Output: {result.output}\")\n",
    "print(f\"Halted: {result.halted}\")\n",
    "print(f\"Candidates evaluated: {result.candidates_evaluated}\")\n",
    "if result.coherence:\n",
    "    print(f\"Coherence: {result.coherence.score:.3f}\")\n",
    "    print(f\"Approved: {result.coherence.approved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. With a real LLM\n",
    "\n",
    "Point the agent at any OpenAI-compatible endpoint (llama.cpp, vLLM, Ollama, etc.):\n",
    "\n",
    "```python\n",
    "agent = CoherenceAgent(llm_api_url=\"http://localhost:8080/completion\")\n",
    "result = agent.process(\"Explain quantum entanglement\")\n",
    "```\n",
    "\n",
    "For NLI-based scoring (detects contradiction in any domain, not just pre-loaded facts):\n",
    "\n",
    "```python\n",
    "!pip install director-ai[nli]  # adds torch + transformers\n",
    "\n",
    "scorer = CoherenceScorer(use_nli=True, threshold=0.6, ground_truth_store=store)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[GitHub](https://github.com/anulum/director-ai) |\n",
    "[PyPI](https://pypi.org/project/director-ai/) |\n",
    "[Docs](https://director-ai.readthedocs.io/) |\n",
    "AGPL-3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
