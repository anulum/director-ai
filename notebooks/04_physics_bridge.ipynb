{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# 04 — Physics Bridge (L16 → Consumer)\n", "\n", "Demonstrates `PhysicsBackedScorer` which blends heuristic coherence\n", "with L16 physics simulation scores."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from director_ai.core.bridge import PhysicsBackedScorer"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Basic Usage\n", "The bridge gracefully degrades when research deps are absent."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scorer = PhysicsBackedScorer(physics_weight=0.3, simulation_steps=20)\n", "print(f\"Physics available: {scorer._oversight is not None}\")\n", "\n", "# Score a claim\n", "score = scorer.score(\"The sky is blue\", \"sky is blue\")\n", "print(f\"Score: {score}\")"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Full Review Pipeline"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = scorer.review(\"Earth orbits the Sun\", \"planets orbit stars\")\n", "print(f\"Coherence: {result.coherence}\")\n", "print(f\"Approved:  {result.approved}\")\n", "print(f\"Reason:    {result.reason}\")"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing Weights\n", "See how `physics_weight` affects the final score."]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for w in [0.0, 0.3, 0.5, 0.8, 1.0]:\n", "    s = PhysicsBackedScorer(physics_weight=w)\n", "    score = s.score(\"The sky is blue\", \"sky is blue\")\n", "    print(f\"  weight={w:.1f} → score={score:.4f}\")"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.0"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
