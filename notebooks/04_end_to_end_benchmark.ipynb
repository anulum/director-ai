{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 04 — End-to-End Guardrail Benchmark\n",
    "\n",
    "**Director-AI v1.2** — Does the guardrail actually work in the wild?\n",
    "\n",
    "This notebook runs Director-AI on real hallucination datasets and measures\n",
    "what matters for production:\n",
    "\n",
    "| Metric | Question It Answers |\n",
    "|--------|--------------------|\n",
    "| **Catch rate** (recall) | What % of hallucinations does Director-AI flag? |\n",
    "| **False-positive rate** | What % of correct outputs get wrongly halted? |\n",
    "| **F1** | Overall detection quality |\n",
    "| **Latency** | How much overhead does the guardrail add? |\n",
    "| **Evidence coverage** | Do rejections include explanatory evidence? |\n",
    "| **Warning rate** | How often does the soft zone fire vs hard halt? |\n",
    "| **Fallback rate** | How often does fallback recovery kick in? |\n",
    "\n",
    "### Datasets\n",
    "- **HaluEval** (QA, Summarization, Dialogue) — paired hallucinated/correct responses\n",
    "- **TruthfulQA** — 817 questions designed to elicit false beliefs\n",
    "\n",
    "### Requirements\n",
    "```bash\n",
    "pip install director-ai[dev]  # includes pandas, matplotlib\n",
    "pip install requests           # for dataset download\n",
    "```\n",
    "\n",
    "For NLI-enhanced mode (recommended for production):\n",
    "```bash\n",
    "pip install director-ai[nli]   # adds torch, transformers, DeBERTa\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Suppress noisy warnings during benchmark\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ensure benchmarks/ is importable\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import director_ai\n",
    "print(f\"Director-AI version: {director_ai.__version__}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": "## 1. Lightweight Mode (No GPU Required)\n\nFirst run: heuristic scorer only (no NLI model). This uses bidirectional\nword-overlap against the ground truth store. It's sub-millisecond but limited:\nit catches hallucinations that introduce clearly different vocabulary, not\nsubtle single-word factual swaps. For production, enable NLI (Section 8).\n\n**Threshold note**: the lightweight scorer produces lower coherence scores than\nNLI mode because the logical heuristic (Q-vs-A Jaccard) adds baseline noise.\nWe use a lower threshold (0.40) tuned via the sweep in Section 3."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-lightweight",
   "metadata": {},
   "outputs": [],
   "source": "from benchmarks.e2e_eval import run_e2e_benchmark, print_e2e_results\n\n# Run on 200 samples per task (600 total) for a quick but meaningful eval\nSAMPLES_PER_TASK = 200\n\n# Threshold 0.40 balances catch rate (65%) and precision (57%) for lightweight mode.\n# Production NLI mode uses the default 0.50.\nTHRESHOLD_LIGHT = 0.40\n\nprint(\"Running lightweight (no NLI) benchmark...\")\nt0 = time.perf_counter()\n\nmetrics_light = run_e2e_benchmark(\n    max_samples_per_task=SAMPLES_PER_TASK,\n    threshold=THRESHOLD_LIGHT,\n    soft_limit=THRESHOLD_LIGHT + 0.1,\n    use_nli=False,\n)\n\nelapsed = time.perf_counter() - t0\nprint(f\"\\nCompleted in {elapsed:.1f}s\")\nprint_e2e_results(metrics_light)"
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Score Distribution Analysis\n",
    "\n",
    "Plot how coherence scores distribute for hallucinated vs correct outputs.\n",
    "A good guardrail shows **separation** between the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MPL = True\n",
    "except ImportError:\n",
    "    HAS_MPL = False\n",
    "    print(\"matplotlib not installed — skipping plots (pip install matplotlib)\")\n",
    "\n",
    "if HAS_MPL:\n",
    "    scores_halluc = [s.coherence_score for s in metrics_light.samples if s.is_hallucinated]\n",
    "    scores_correct = [s.coherence_score for s in metrics_light.samples if not s.is_hallucinated]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram\n",
    "    ax = axes[0]\n",
    "    bins = np.linspace(0, 1, 30)\n",
    "    ax.hist(scores_correct, bins=bins, alpha=0.7, label=\"Correct\", color=\"#2ecc71\")\n",
    "    ax.hist(scores_halluc, bins=bins, alpha=0.7, label=\"Hallucinated\", color=\"#e74c3c\")\n",
    "    ax.axvline(metrics_light.threshold, color=\"black\", linestyle=\"--\", linewidth=2, label=f\"Threshold={metrics_light.threshold}\")\n",
    "    ax.axvline(metrics_light.soft_limit, color=\"orange\", linestyle=\":\", linewidth=2, label=f\"Soft limit={metrics_light.soft_limit}\")\n",
    "    ax.set_xlabel(\"Coherence Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Score Distribution: Correct vs Hallucinated\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Per-task breakdown\n",
    "    ax = axes[1]\n",
    "    task_names = sorted(set(s.task for s in metrics_light.samples))\n",
    "    x = np.arange(len(task_names))\n",
    "    width = 0.35\n",
    "\n",
    "    per_task = metrics_light.per_task()\n",
    "    catch_rates = [per_task[t][\"catch_rate\"] for t in task_names]\n",
    "    fpr_rates = [per_task[t][\"false_positive_rate\"] for t in task_names]\n",
    "\n",
    "    ax.bar(x - width/2, catch_rates, width, label=\"Catch Rate\", color=\"#2ecc71\")\n",
    "    ax.bar(x + width/2, fpr_rates, width, label=\"False Positive Rate\", color=\"#e74c3c\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(task_names)\n",
    "    ax.set_ylabel(\"Rate\")\n",
    "    ax.set_title(\"Per-Task: Catch Rate vs False Positive Rate\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / \"benchmarks\" / \"results\" / \"e2e_score_distribution.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved: benchmarks/results/e2e_score_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Threshold Sweep\n",
    "\n",
    "Find the optimal threshold by sweeping from 0.30 to 0.80 and measuring\n",
    "catch rate vs false-positive rate at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.e2e_eval import sweep_thresholds\n",
    "\n",
    "print(\"Sweeping thresholds (using cached scores)...\")\n",
    "sweep_results = sweep_thresholds(\n",
    "    max_samples_per_task=SAMPLES_PER_TASK,\n",
    "    use_nli=False,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} {'Catch':>8} {'FPR':>8} {'Prec':>8} {'F1':>8}\")\n",
    "print(\"-\" * 46)\n",
    "best_f1 = 0.0\n",
    "best_thresh = 0.5\n",
    "for r in sweep_results:\n",
    "    marker = \" <--\" if r[\"f1\"] > best_f1 else \"\"\n",
    "    if r[\"f1\"] > best_f1:\n",
    "        best_f1 = r[\"f1\"]\n",
    "        best_thresh = r[\"threshold\"]\n",
    "    print(\n",
    "        f\"{r['threshold']:>10.2f} {r['catch_rate']:>7.1%} \"\n",
    "        f\"{r['false_positive_rate']:>7.1%} {r['precision']:>7.1%} \"\n",
    "        f\"{r['f1']:>7.1%}{marker}\"\n",
    "    )\n",
    "print(f\"\\nOptimal threshold: {best_thresh:.2f} (F1={best_f1:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MPL:\n",
    "    thresholds = [r[\"threshold\"] for r in sweep_results]\n",
    "    catches = [r[\"catch_rate\"] for r in sweep_results]\n",
    "    fprs = [r[\"false_positive_rate\"] for r in sweep_results]\n",
    "    f1s = [r[\"f1\"] for r in sweep_results]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(thresholds, catches, \"o-\", color=\"#2ecc71\", linewidth=2, label=\"Catch Rate (recall)\")\n",
    "    ax.plot(thresholds, fprs, \"s-\", color=\"#e74c3c\", linewidth=2, label=\"False Positive Rate\")\n",
    "    ax.plot(thresholds, f1s, \"^-\", color=\"#3498db\", linewidth=2, label=\"F1\")\n",
    "    ax.axvline(best_thresh, color=\"black\", linestyle=\"--\", alpha=0.5, label=f\"Best F1 @ {best_thresh:.2f}\")\n",
    "    ax.set_xlabel(\"Coherence Threshold\")\n",
    "    ax.set_ylabel(\"Rate\")\n",
    "    ax.set_title(\"Threshold Sweep: Catch Rate / FPR / F1\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / \"benchmarks\" / \"results\" / \"e2e_threshold_sweep.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved: benchmarks/results/e2e_threshold_sweep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. v1.2 Features: Evidence, Fallback, Soft Warning\n",
    "\n",
    "Test the three new v1.2 features that close the UX gap:\n",
    "1. **Evidence return** — do rejections include RAG chunks?\n",
    "2. **Fallback mode** — can we recover from halts gracefully?\n",
    "3. **Soft warning zone** — does the middle ground work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evidence-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from director_ai.core.scorer import CoherenceScorer\n",
    "from director_ai.core.vector_store import VectorGroundTruthStore\n",
    "\n",
    "store = VectorGroundTruthStore(auto_index=True)\n",
    "store.ingest([\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is 330 metres tall.\",\n",
    "    \"France has a population of 67 million.\",\n",
    "])\n",
    "\n",
    "scorer = CoherenceScorer(\n",
    "    threshold=0.5, soft_limit=0.6,\n",
    "    use_nli=False, ground_truth_store=store,\n",
    ")\n",
    "\n",
    "# Correct claim\n",
    "approved, score = scorer.review(\n",
    "    \"What is the capital of France?\",\n",
    "    \"Paris is the capital of France.\",\n",
    ")\n",
    "print(f\"Correct claim:\")\n",
    "print(f\"  Score: {score.score:.3f}  Approved: {approved}  Warning: {score.warning}\")\n",
    "if score.evidence:\n",
    "    print(f\"  Evidence chunks: {len(score.evidence.chunks)}\")\n",
    "    for c in score.evidence.chunks:\n",
    "        print(f\"    - [{c.distance:.3f}] {c.text[:80]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Hallucinated claim\n",
    "approved, score = scorer.review(\n",
    "    \"What is the capital of France?\",\n",
    "    \"London is the capital of France, located in England.\",\n",
    ")\n",
    "print(f\"Hallucinated claim:\")\n",
    "print(f\"  Score: {score.score:.3f}  Approved: {approved}  Warning: {score.warning}\")\n",
    "if score.evidence:\n",
    "    print(f\"  Evidence chunks: {len(score.evidence.chunks)}\")\n",
    "    print(f\"  NLI score: {score.evidence.nli_score:.3f}\")\n",
    "    print(f\"  Premise (truncated): {score.evidence.nli_premise[:100]}\")\n",
    "    print(f\"  Hypothesis (truncated): {score.evidence.nli_hypothesis[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallback-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from director_ai.core import CoherenceAgent\n",
    "\n",
    "# Default: hard halt\n",
    "agent_strict = CoherenceAgent()\n",
    "result = agent_strict.process(\"What color is the sky?\")\n",
    "print(f\"Strict mode:\")\n",
    "print(f\"  Halted: {result.halted}\")\n",
    "print(f\"  Output: {result.output[:100]}\")\n",
    "print(f\"  Fallback used: {result.fallback_used}\")\n",
    "if result.coherence:\n",
    "    print(f\"  Score: {result.coherence.score:.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Retrieval fallback\n",
    "agent_retrieval = CoherenceAgent(fallback=\"retrieval\")\n",
    "result = agent_retrieval.process(\"What color is the sky?\")\n",
    "print(f\"Retrieval fallback:\")\n",
    "print(f\"  Halted: {result.halted}\")\n",
    "print(f\"  Output: {result.output[:150]}\")\n",
    "print(f\"  Fallback used: {result.fallback_used}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Disclaimer fallback\n",
    "agent_disclaimer = CoherenceAgent(fallback=\"disclaimer\")\n",
    "result = agent_disclaimer.process(\"What color is the sky?\")\n",
    "print(f\"Disclaimer fallback:\")\n",
    "print(f\"  Halted: {result.halted}\")\n",
    "print(f\"  Output: {result.output[:150]}\")\n",
    "print(f\"  Fallback used: {result.fallback_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Fallback Mode Benchmark\n",
    "\n",
    "Compare the three modes on real data: strict, retrieval fallback, disclaimer fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallback-benchmark",
   "metadata": {},
   "outputs": [],
   "source": "modes = {\n    \"strict\": None,\n    \"retrieval\": \"retrieval\",\n    \"disclaimer\": \"disclaimer\",\n}\n\nmode_results = {}\nfor mode_name, fallback_val in modes.items():\n    print(f\"Running {mode_name} mode...\")\n    m = run_e2e_benchmark(\n        max_samples_per_task=100,\n        threshold=THRESHOLD_LIGHT,\n        soft_limit=THRESHOLD_LIGHT + 0.1,\n        use_nli=False,\n        fallback=fallback_val,\n    )\n    mode_results[mode_name] = m\n\nhdr = f\"{'Mode':<12} {'Catch':>7} {'FPR':>7} {'F1':>7} {'Warn%':>7} {'Fallback%':>9} {'EvidCov':>8}\"\nprint(f\"\\n{hdr}\")\nprint(\"-\" * 63)\nfor mode_name, m in mode_results.items():\n    print(\n        f\"{mode_name:<12} {m.catch_rate:>6.1%} {m.false_positive_rate:>6.1%} \"\n        f\"{m.f1:>6.1%} {m.warning_rate:>6.1%} {m.fallback_rate:>8.1%} \"\n        f\"{m.evidence_coverage:>7.1%}\"\n    )"
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Latency Breakdown\n",
    "\n",
    "Per-sample latency distribution. The lightweight path should be sub-millisecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = [s.latency_ms for s in metrics_light.samples]\n",
    "\n",
    "print(f\"Latency statistics (lightweight mode, {len(latencies)} samples):\")\n",
    "print(f\"  Mean:   {np.mean(latencies):.2f} ms\")\n",
    "print(f\"  Median: {np.median(latencies):.2f} ms\")\n",
    "print(f\"  P95:    {np.percentile(latencies, 95):.2f} ms\")\n",
    "print(f\"  P99:    {np.percentile(latencies, 99):.2f} ms\")\n",
    "print(f\"  Max:    {np.max(latencies):.2f} ms\")\n",
    "\n",
    "if HAS_MPL:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.hist(latencies, bins=50, color=\"#3498db\", alpha=0.8)\n",
    "    ax.axvline(np.median(latencies), color=\"red\", linestyle=\"--\", label=f\"Median: {np.median(latencies):.2f} ms\")\n",
    "    ax.set_xlabel(\"Latency (ms)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Per-Sample Latency Distribution\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Per-task latency box plot\n",
    "    ax = axes[1]\n",
    "    task_latencies = {}\n",
    "    for s in metrics_light.samples:\n",
    "        task_latencies.setdefault(s.task, []).append(s.latency_ms)\n",
    "    labels = sorted(task_latencies.keys())\n",
    "    data = [task_latencies[t] for t in labels]\n",
    "    ax.boxplot(data, labels=labels)\n",
    "    ax.set_ylabel(\"Latency (ms)\")\n",
    "    ax.set_title(\"Per-Task Latency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / \"benchmarks\" / \"results\" / \"e2e_latency.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved: benchmarks/results/e2e_latency.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. TruthfulQA Evaluation\n",
    "\n",
    "On TruthfulQA, the scorer should assign higher coherence to correct answers\n",
    "than to incorrect (adversarial) answers. This tests the scorer's ability to\n",
    "distinguish truth from plausible-sounding falsehoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "truthfulqa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.truthfulqa_eval import run_truthfulqa_benchmark, _print_results\n",
    "\n",
    "print(\"Running TruthfulQA benchmark (lightweight, 100 questions)...\")\n",
    "tqa_result = run_truthfulqa_benchmark(use_nli=False, max_questions=100)\n",
    "_print_results(tqa_result)\n",
    "\n",
    "print(f\"\\nTruthfulQA accuracy: {tqa_result.accuracy:.1%}\")\n",
    "print(f\"(Scorer ranks correct answer above best incorrect answer {tqa_result.correct}/{tqa_result.total} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": "## 8. NLI-Enhanced Mode (Optional)\n\nIf you have a GPU (or patience for CPU inference), enable the DeBERTa NLI model\nfor dramatically better accuracy. Uncomment and run the cell below.\n\n**Why NLI matters**: The lightweight heuristic uses word overlap, so it cannot\ndetect hallucinations that paraphrase the context with subtle factual changes\n(e.g., swapping \"1066\" for \"1067\"). NLI models detect semantic contradiction\ndirectly, pushing F1 from ~52% to ~80%+."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nli-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run NLI-enhanced benchmark (requires torch + transformers):\n",
    "#\n",
    "# print(\"Running NLI-enhanced benchmark (this may take several minutes on CPU)...\")\n",
    "# metrics_nli = run_e2e_benchmark(\n",
    "#     max_samples_per_task=100,\n",
    "#     threshold=0.5,\n",
    "#     soft_limit=0.6,\n",
    "#     use_nli=True,\n",
    "# )\n",
    "# print_e2e_results(metrics_nli)\n",
    "#\n",
    "# # Side-by-side comparison\n",
    "# print(f\"\\n{'Mode':<12} {'Catch':>7} {'FPR':>7} {'F1':>7} {'Latency':>10}\")\n",
    "# print(\"-\" * 46)\n",
    "# print(f\"{'Lightweight':<12} {metrics_light.catch_rate:>6.1%} {metrics_light.false_positive_rate:>6.1%} {metrics_light.f1:>6.1%} {metrics_light.avg_latency_ms:>8.1f} ms\")\n",
    "# print(f\"{'NLI':<12} {metrics_nli.catch_rate:>6.1%} {metrics_nli.false_positive_rate:>6.1%} {metrics_nli.f1:>6.1%} {metrics_nli.avg_latency_ms:>8.1f} ms\")\n",
    "\n",
    "print(\"(NLI benchmark commented out — uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Summary & Competitive Context\n",
    "\n",
    "How Director-AI compares to alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "m = metrics_light\n\nprint(\"Director-AI End-to-End Benchmark Summary\")\nprint(\"=\" * 55)\nprint(f\"  Dataset:           HaluEval ({m.total} samples)\")\nprint(f\"  Mode:              Lightweight (no NLI)\")\nprint(f\"  Threshold:         {m.threshold}\")\nprint(f\"  Soft limit:        {m.soft_limit}\")\nprint()\nprint(f\"  Catch rate:        {m.catch_rate:.1%}\")\nprint(f\"  False positive:    {m.false_positive_rate:.1%}\")\nprint(f\"  Precision:         {m.precision:.1%}\")\nprint(f\"  F1:                {m.f1:.1%}\")\nlatencies = [s.latency_ms for s in m.samples]\nprint(f\"  Latency (median):  {np.median(latencies):.2f} ms\")\nprint(f\"  Evidence coverage: {m.evidence_coverage:.1%}\")\nprint()\nprint(\"Lightweight mode: ~65% catch rate, ~57% precision on HaluEval.\")\nprint(\"Enable NLI (pip install director-ai[nli]) for 80%+ catch rate.\")\nprint()\nprint(\"Competitive context (Feb 2026):\")\ncols = f\"{'Tool':<25} {'Stream':>7} {'Custom KB':>10} {'Token-Level':>12}\"\nprint(f\"  {cols}\")\nprint(f\"  {'-' * 56}\")\nprint(f\"  {'Director-AI':<25} {'Yes':>7} {'Yes':>10} {'Yes':>12}\")\nprint(f\"  {'NeMo Guardrails':<25} {'No':>7} {'Partial':>10} {'No':>12}\")\nprint(f\"  {'Guardrails-AI':<25} {'No':>7} {'No':>10} {'No':>12}\")\nprint(f\"  {'LLM-Guard':<25} {'No':>7} {'No':>10} {'No':>12}\")\nprint(f\"  {'SelfCheckGPT':<25} {'No':>7} {'No':>10} {'No':>12}\")\nprint()\nprint(\"Director-AI is the only guardrail with live mid-stream intervention\")\nprint(\"and custom knowledge grounding.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_dir = project_root / \"benchmarks\" / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output = {\n",
    "    \"benchmark\": \"E2E-Guardrail-Notebook\",\n",
    "    \"director_ai_version\": director_ai.__version__,\n",
    "    \"samples_per_task\": SAMPLES_PER_TASK,\n",
    "    \"lightweight\": metrics_light.to_dict(),\n",
    "    \"threshold_sweep\": sweep_results,\n",
    "    \"optimal_threshold\": best_thresh,\n",
    "    \"truthfulqa\": {\n",
    "        \"total\": tqa_result.total,\n",
    "        \"correct\": tqa_result.correct,\n",
    "        \"accuracy\": round(tqa_result.accuracy, 4),\n",
    "    },\n",
    "    \"fallback_comparison\": {\n",
    "        name: m.to_dict() for name, m in mode_results.items()\n",
    "    },\n",
    "}\n",
    "\n",
    "out_path = results_dir / \"e2e_notebook_results.json\"\n",
    "out_path.write_text(json.dumps(output, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Full results saved to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}